{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5551476c",
   "metadata": {},
   "source": [
    "To run this, press \"*Runtime*\" and press \"*Run all*\" on a **free** Tesla T4 Google Colab instance!\n",
    "<div class=\"align-center\">\n",
    "<a href=\"https://unsloth.ai/\"><img src=\"https://github.com/unslothai/unsloth/raw/main/images/unsloth%20new%20logo.png\" width=\"115\"></a>\n",
    "<a href=\"https://discord.gg/unsloth\"><img src=\"https://github.com/unslothai/unsloth/raw/main/images/Discord button.png\" width=\"145\"></a>\n",
    "<a href=\"https://docs.unsloth.ai/\"><img src=\"https://github.com/unslothai/unsloth/blob/main/images/documentation%20green%20button.png?raw=true\" width=\"125\"></a></a> Join Discord if you need help + ‚≠ê <i>Star us on <a href=\"https://github.com/unslothai/unsloth\">Github</a> </i> ‚≠ê\n",
    "</div>\n",
    "\n",
    "To install Unsloth on your own computer, follow the installation instructions on our Github page [here](https://docs.unsloth.ai/get-started/installing-+-updating).\n",
    "\n",
    "You will learn how to do [data prep](#Data), how to [train](#Train), how to [run the model](#Inference), & [how to save it](#Save)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7dc9931",
   "metadata": {},
   "source": [
    "### News"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "406aa7b4",
   "metadata": {},
   "source": [
    "Read our **[Qwen3 Guide](https://docs.unsloth.ai/basics/qwen3-how-to-run-and-fine-tune)** and check out our new **[Dynamic 2.0](https://docs.unsloth.ai/basics/unsloth-dynamic-2.0-ggufs)** quants which outperforms other quantization methods!\n",
    "\n",
    "Visit our docs for all our [model uploads](https://docs.unsloth.ai/get-started/all-our-models) and [notebooks](https://docs.unsloth.ai/get-started/unsloth-notebooks).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30002805",
   "metadata": {},
   "source": [
    "### Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "89597fd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "import os\n",
    "if \"COLAB_\" not in \"\".join(os.environ.keys()):\n",
    "    !pip install unsloth\n",
    "else:\n",
    "    # Do this only in Colab notebooks! Otherwise use pip install unsloth\n",
    "    !pip install --no-deps bitsandbytes accelerate xformers==0.0.29.post3 peft trl==0.15.2 triton cut_cross_entropy unsloth_zoo\n",
    "    !pip install sentencepiece protobuf datasets huggingface_hub hf_transfer\n",
    "    !pip install --no-deps unsloth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d22c1eee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.7.0+cu126 12.6\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "print(torch.__version__, torch.version.cuda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "98e3e48b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.12.9\n",
      "nvcc: NVIDIA (R) Cuda compiler driver\n",
      "Copyright (c) 2005-2024 NVIDIA Corporation\n",
      "Built on Thu_Jun__6_02:18:23_PDT_2024\n",
      "Cuda compilation tools, release 12.5, V12.5.82\n",
      "Build cuda_12.5.r12.5/compiler.34385749_0\n",
      "Wed May  7 00:24:30 2025       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 570.133.20             Driver Version: 570.133.20     CUDA Version: 12.8     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA GeForce RTX 2080 Ti     On  |   00000000:08:00.0 Off |                  N/A |\n",
      "| 53%   49C    P8             30W /  260W |     325MiB /  22528MiB |      1%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|    0   N/A  N/A            2404      G   /usr/lib/xorg/Xorg                      166MiB |\n",
      "|    0   N/A  N/A            2526      G   /usr/bin/sddm-greeter                   123MiB |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!python --version\n",
    "!nvcc --version\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7e65a72",
   "metadata": {},
   "source": [
    "### Unsloth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c2ac1cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ü¶• Unsloth: Will patch your computer to enable 2x faster free finetuning.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/seven/tool/miniconda3/envs/unsloth/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ü¶• Unsloth Zoo will now patch everything to make training faster!\n",
      "==((====))==  Unsloth 2025.4.7: Fast Qwen3 patching. Transformers: 4.51.3.\n",
      "   \\\\   /|    NVIDIA GeForce RTX 2080 Ti. Num GPUs = 1. Max memory: 21.483 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.7.0+cu126. CUDA: 7.5. CUDA Toolkit: 12.6. Triton: 3.3.0\n",
      "\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.30. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4/4 [00:09<00:00,  2.43s/it]\n"
     ]
    }
   ],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "import torch\n",
    "\n",
    "fourbit_models = [\n",
    "    \"unsloth/Qwen3-1.7B-unsloth-bnb-4bit\", # Qwen 14B 2x faster\n",
    "    \"unsloth/Qwen3-4B-unsloth-bnb-4bit\",\n",
    "    \"unsloth/Qwen3-8B-unsloth-bnb-4bit\",\n",
    "    \"unsloth/Qwen3-14B-unsloth-bnb-4bit\",\n",
    "    \"unsloth/Qwen3-32B-unsloth-bnb-4bit\",\n",
    "\n",
    "    # 4bit dynamic quants for superior accuracy and low memory use\n",
    "    \"unsloth/gemma-3-12b-it-unsloth-bnb-4bit\",\n",
    "    \"unsloth/Phi-4\",\n",
    "    \"unsloth/Llama-3.1-8B\",\n",
    "    \"unsloth/Llama-3.2-3B\",\n",
    "    \"unsloth/orpheus-3b-0.1-ft-unsloth-bnb-4bit\" # [NEW] We support TTS models!\n",
    "] # More models at https://huggingface.co/unsloth\n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = \"/mnt/data1/D2/unsloth/Qwen3-8B\",\n",
    "    max_seq_length = 4096,   # Context length - can be longer, but uses more memory\n",
    "    load_in_4bit = True,     # 4bit uses much less memory\n",
    "    load_in_8bit = False,    # A bit more accurate, uses 2x memory\n",
    "    full_finetuning = False, # We have full finetuning now!\n",
    "    # token = \"hf_...\",      # use one if using gated models\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42ddc8f9",
   "metadata": {},
   "source": [
    "We now add LoRA adapters so we only need to update 1 to 10% of all parameters!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8d421f2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth: Dropout = 0 is supported for fast patching. You are using dropout = 0.05.\n",
      "Unsloth will patch all other layers, except LoRA matrices, causing a performance hit.\n",
      "Unsloth 2025.4.7 patched 36 layers with 0 QKV layers, 0 O layers and 0 MLP layers.\n"
     ]
    }
   ],
   "source": [
    "# model = FastLanguageModel.get_peft_model(\n",
    "#     model,\n",
    "#     r = 32,           # Choose any number > 0! Suggested 8, 16, 32, 64, 128\n",
    "#     target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "#                       \"gate_proj\", \"up_proj\", \"down_proj\",],\n",
    "#     lora_alpha = 32,  # Best to choose alpha = rank or rank*2\n",
    "#     lora_dropout = 0, # Supports any, but = 0 is optimized\n",
    "#     bias = \"none\",    # Supports any, but = \"none\" is optimized\n",
    "#     # [NEW] \"unsloth\" uses 30% less VRAM, fits 2x larger batch sizes!\n",
    "#     use_gradient_checkpointing = \"unsloth\", # True or \"unsloth\" for very long context\n",
    "#     random_state = 3407,\n",
    "#     use_rslora = False,   # We support rank stabilized LoRA\n",
    "#     loftq_config = None,  # And LoftQ\n",
    "# )\n",
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.05,\n",
    "    target_modules=[\"q_proj\", \"v_proj\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92533040",
   "metadata": {},
   "source": [
    "<a name=\"Data\"></a>\n",
    "### Data Prep\n",
    "Qwen3 has both reasoning and a non reasoning mode. So, we should use 2 datasets:\n",
    "\n",
    "1. We use the [Open Math Reasoning]() dataset which was used to win the [AIMO](https://www.kaggle.com/competitions/ai-mathematical-olympiad-progress-prize-2/leaderboard) (AI Mathematical Olympiad - Progress Prize 2) challenge! We sample 10% of verifiable reasoning traces that used DeepSeek R1, and whicht got > 95% accuracy.\n",
    "\n",
    "2. We also leverage [Maxime Labonne's FineTome-100k](https://huggingface.co/datasets/mlabonne/FineTome-100k) dataset in ShareGPT style. But we need to convert it to HuggingFace's normal multiturn format as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dc149212",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "reasoning_dataset = load_dataset(\"/mnt/data1/D2/unsloth/OpenMathReasoning-mini\", split = \"cot\")\n",
    "non_reasoning_dataset = load_dataset(\"/mnt/data1/D2/mlabonne/FineTome-100k\", split = \"train\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8be93f22",
   "metadata": {},
   "source": [
    "Let's see the structure of both datasets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1d706166",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['expected_answer', 'problem_type', 'problem_source', 'generation_model', 'pass_rate_72b_tir', 'problem', 'generated_solution', 'inference_mode'],\n",
       "    num_rows: 19252\n",
       "})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reasoning_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0b56296b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['conversations', 'source', 'score'],\n",
       "    num_rows: 100000\n",
       "})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "non_reasoning_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6b528f0",
   "metadata": {},
   "source": [
    "We now convert the reasoning dataset into conversational format:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "90ed8fe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_conversation(examples):\n",
    "    problems  = examples[\"problem\"]\n",
    "    solutions = examples[\"generated_solution\"]\n",
    "    conversations = []\n",
    "    for problem, solution in zip(problems, solutions):\n",
    "        conversations.append([\n",
    "            {\"role\" : \"user\",      \"content\" : problem},\n",
    "            {\"role\" : \"assistant\", \"content\" : solution},\n",
    "        ])\n",
    "    return { \"conversations\": conversations, }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3b92a3c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "reasoning_conversations = tokenizer.apply_chat_template(\n",
    "    reasoning_dataset.map(generate_conversation, batched = True)[\"conversations\"],\n",
    "    tokenize = False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84fe1444",
   "metadata": {},
   "source": [
    "Let's see the first transformed row:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e418288e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"<|im_start|>user\\nGiven $\\\\sqrt{x^2+165}-\\\\sqrt{x^2-52}=7$ and $x$ is positive, find all possible values of $x$.<|im_end|>\\n<|im_start|>assistant\\n<think>\\nOkay, let's see. I need to solve the equation ‚àö(x¬≤ + 165) - ‚àö(x¬≤ - 52) = 7, and find all positive values of x. Hmm, radicals can be tricky, but maybe if I can eliminate the square roots by squaring both sides. Let me try that.\\n\\nFirst, let me write down the equation again to make sure I have it right:\\n\\n‚àö(x¬≤ + 165) - ‚àö(x¬≤ - 52) = 7.\\n\\nOkay, so the idea is to isolate one of the radicals and then square both sides. Let me try moving the second radical to the other side:\\n\\n‚àö(x¬≤ + 165) = 7 + ‚àö(x¬≤ - 52).\\n\\nNow, if I square both sides, maybe I can get rid of the square roots. Let's do that:\\n\\n(‚àö(x¬≤ + 165))¬≤ = (7 + ‚àö(x¬≤ - 52))¬≤.\\n\\nSimplifying the left side:\\n\\nx¬≤ + 165 = 49 + 14‚àö(x¬≤ - 52) + (‚àö(x¬≤ - 52))¬≤.\\n\\nThe right side is expanded using the formula (a + b)¬≤ = a¬≤ + 2ab + b¬≤. So the right side becomes 7¬≤ + 2*7*‚àö(x¬≤ - 52) + (‚àö(x¬≤ - 52))¬≤, which is 49 + 14‚àö(x¬≤ - 52) + (x¬≤ - 52).\\n\\nSo putting it all together:\\n\\nx¬≤ + 165 = 49 + 14‚àö(x¬≤ - 52) + x¬≤ - 52.\\n\\nHmm, let's simplify the right side. The x¬≤ terms will cancel out, right? Let's subtract x¬≤ from both sides:\\n\\n165 = 49 + 14‚àö(x¬≤ - 52) - 52.\\n\\nSimplify the constants on the right:\\n\\n49 - 52 is -3, so:\\n\\n165 = -3 + 14‚àö(x¬≤ - 52).\\n\\nNow, add 3 to both sides to isolate the radical term:\\n\\n165 + 3 = 14‚àö(x¬≤ - 52).\\n\\nSo 168 = 14‚àö(x¬≤ - 52).\\n\\nDivide both sides by 14:\\n\\n168 / 14 = ‚àö(x¬≤ - 52).\\n\\n12 = ‚àö(x¬≤ - 52).\\n\\nNow, square both sides again to eliminate the square root:\\n\\n12¬≤ = x¬≤ - 52.\\n\\n144 = x¬≤ - 52.\\n\\nAdd 52 to both sides:\\n\\n144 + 52 = x¬≤.\\n\\n196 = x¬≤.\\n\\nSo x = ‚àö196 = 14.\\n\\nBut wait, since the problem states that x is positive, we only take the positive root. So x = 14.\\n\\nBut hold on, when dealing with squaring equations, sometimes extraneous solutions can come up. I should check if this solution actually satisfies the original equation.\\n\\nLet's plug x = 14 back into the original equation:\\n\\n‚àö(14¬≤ + 165) - ‚àö(14¬≤ - 52) = ?\\n\\nCalculate each term:\\n\\n14¬≤ is 196.\\n\\nSo first radical: ‚àö(196 + 165) = ‚àö361 = 19.\\n\\nSecond radical: ‚àö(196 - 52) = ‚àö144 = 12.\\n\\nSo 19 - 12 = 7, which is exactly the right-hand side. So yes, it checks out.\\n\\nTherefore, the only solution is x = 14. Since the problem says x is positive, we don't have to consider negative roots. So I think that's the answer.\\n</think>\\n\\nTo solve the equation \\\\(\\\\sqrt{x^2 + 165} - \\\\sqrt{x^2 - 52} = 7\\\\) for positive \\\\(x\\\\), we proceed as follows:\\n\\n1. Start with the given equation:\\n   \\\\[\\n   \\\\sqrt{x^2 + 165} - \\\\sqrt{x^2 - 52} = 7\\n   \\\\]\\n\\n2. Isolate one of the square roots by moving \\\\(\\\\sqrt{x^2 - 52}\\\\) to the right side:\\n   \\\\[\\n   \\\\sqrt{x^2 + 165} = 7 + \\\\sqrt{x^2 - 52}\\n   \\\\]\\n\\n3. Square both sides to eliminate the square root on the left:\\n   \\\\[\\n   (\\\\sqrt{x^2 + 165})^2 = (7 + \\\\sqrt{x^2 - 52})^2\\n   \\\\]\\n   Simplifying both sides, we get:\\n   \\\\[\\n   x^2 + 165 = 49 + 14\\\\sqrt{x^2 - 52} + (x^2 - 52)\\n   \\\\]\\n\\n4. Combine like terms on the right side:\\n   \\\\[\\n   x^2 + 165 = x^2 - 52 + 49 + 14\\\\sqrt{x^2 - 52}\\n   \\\\]\\n   Simplifying further:\\n   \\\\[\\n   x^2 + 165 = x^2 - 3 + 14\\\\sqrt{x^2 - 52}\\n   \\\\]\\n\\n5. Subtract \\\\(x^2\\\\) from both sides:\\n   \\\\[\\n   165 = -3 + 14\\\\sqrt{x^2 - 52}\\n   \\\\]\\n\\n6. Add 3 to both sides to isolate the term with the square root:\\n   \\\\[\\n   168 = 14\\\\sqrt{x^2 - 52}\\n   \\\\]\\n\\n7. Divide both sides by 14:\\n   \\\\[\\n   12 = \\\\sqrt{x^2 - 52}\\n   \\\\]\\n\\n8. Square both sides again to eliminate the square root:\\n   \\\\[\\n   12^2 = x^2 - 52\\n   \\\\]\\n   Simplifying:\\n   \\\\[\\n   144 = x^2 - 52\\n   \\\\]\\n\\n9. Add 52 to both sides to solve for \\\\(x^2\\\\):\\n   \\\\[\\n   196 = x^2\\n   \\\\]\\n\\n10. Take the positive square root (since \\\\(x\\\\) is positive):\\n    \\\\[\\n    x = \\\\sqrt{196} = 14\\n    \\\\]\\n\\n11. Verify the solution by substituting \\\\(x = 14\\\\) back into the original equation:\\n    \\\\[\\n    \\\\sqrt{14^2 + 165} - \\\\sqrt{14^2 - 52} = \\\\sqrt{196 + 165} - \\\\sqrt{196 - 52} = \\\\sqrt{361} - \\\\sqrt{144} = 19 - 12 = 7\\n    \\\\]\\n    The solution checks out.\\n\\nThus, the only positive solution is:\\n\\\\[\\n\\\\boxed{14}\\n\\\\]<|im_end|>\\n\""
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reasoning_conversations[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "984b628a",
   "metadata": {},
   "source": [
    "Next we take the non reasoning dataset and convert it to conversational format as well.\n",
    "\n",
    "We have to use Unsloth's `standardize_sharegpt` function to fix up the format of the dataset first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b2d14658",
   "metadata": {},
   "outputs": [],
   "source": [
    "from unsloth.chat_templates import standardize_sharegpt\n",
    "dataset = standardize_sharegpt(non_reasoning_dataset)\n",
    "\n",
    "non_reasoning_conversations = tokenizer.apply_chat_template(\n",
    "    dataset[\"conversations\"],\n",
    "    tokenize = False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dbf782f",
   "metadata": {},
   "source": [
    "Let's see the first row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "23c2180c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<|im_start|>user\\nExplain what boolean operators are, what they do, and provide examples of how they can be used in programming. Additionally, describe the concept of operator precedence and provide examples of how it affects the evaluation of boolean expressions. Discuss the difference between short-circuit evaluation and normal evaluation in boolean expressions and demonstrate their usage in code. \\n\\nFurthermore, add the requirement that the code must be written in a language that does not support short-circuit evaluation natively, forcing the test taker to implement their own logic for short-circuit evaluation.\\n\\nFinally, delve into the concept of truthiness and falsiness in programming languages, explaining how it affects the evaluation of boolean expressions. Add the constraint that the test taker must write code that handles cases where truthiness and falsiness are implemented differently across different programming languages.<|im_end|>\\n<|im_start|>assistant\\n<think>\\n\\n</think>\\n\\nBoolean operators are logical operators used in programming to manipulate boolean values. They operate on one or more boolean operands and return a boolean result. The three main boolean operators are \"AND\" (&&), \"OR\" (||), and \"NOT\" (!).\\n\\nThe \"AND\" operator returns true if both of its operands are true, and false otherwise. For example:\\n\\n```python\\nx = 5\\ny = 10\\nresult = (x > 0) and (y < 20)  # This expression evaluates to True\\n```\\n\\nThe \"OR\" operator returns true if at least one of its operands is true, and false otherwise. For example:\\n\\n```python\\nx = 5\\ny = 10\\nresult = (x > 0) or (y < 20)  # This expression evaluates to True\\n```\\n\\nThe \"NOT\" operator negates the boolean value of its operand. It returns true if the operand is false, and false if the operand is true. For example:\\n\\n```python\\nx = 5\\nresult = not (x > 10)  # This expression evaluates to True\\n```\\n\\nOperator precedence refers to the order in which operators are evaluated in an expression. It ensures that expressions are evaluated correctly. In most programming languages, logical AND has higher precedence than logical OR. For example:\\n\\n```python\\nresult = True or False and False  # This expression is evaluated as (True or (False and False)), which is True\\n```\\n\\nShort-circuit evaluation is a behavior where the second operand of a logical operator is not evaluated if the result can be determined based on the value of the first operand. In short-circuit evaluation, if the first operand of an \"AND\" operator is false, the second operand is not evaluated because the result will always be false. Similarly, if the first operand of an \"OR\" operator is true, the second operand is not evaluated because the result will always be true.\\n\\nIn programming languages that support short-circuit evaluation natively, you can use it to improve performance or avoid errors. For example:\\n\\n```python\\nif x != 0 and (y / x) > 10:\\n    # Perform some operation\\n```\\n\\nIn languages without native short-circuit evaluation, you can implement your own logic to achieve the same behavior. Here\\'s an example in pseudocode:\\n\\n```\\nif x != 0 {\\n    if (y / x) > 10 {\\n        // Perform some operation\\n    }\\n}\\n```\\n\\nTruthiness and falsiness refer to how non-boolean values are evaluated in boolean contexts. In many programming languages, non-zero numbers and non-empty strings are considered truthy, while zero, empty strings, and null/None values are considered falsy.\\n\\nWhen evaluating boolean expressions, truthiness and falsiness come into play. For example:\\n\\n```python\\nx = 5\\nresult = x  # The value of x is truthy, so result is also truthy\\n```\\n\\nTo handle cases where truthiness and falsiness are implemented differently across programming languages, you can explicitly check the desired condition. For example:\\n\\n```python\\nx = 5\\nresult = bool(x)  # Explicitly converting x to a boolean value\\n```\\n\\nThis ensures that the result is always a boolean value, regardless of the language\\'s truthiness and falsiness rules.<|im_end|>\\n'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "non_reasoning_conversations[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ee28ab7",
   "metadata": {},
   "source": [
    "Now let's see how long both datasets are:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f4470445",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19252\n",
      "100000\n"
     ]
    }
   ],
   "source": [
    "print(len(reasoning_conversations))\n",
    "print(len(non_reasoning_conversations))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68a1137a",
   "metadata": {},
   "source": [
    "The non reasoning dataset is much longer. Let's assume we want the model to retain some reasoning capabilities, but we specifically want a chat model.\n",
    "\n",
    "Let's define a ratio of chat only data. The goal is to define some mixture of both sets of data.\n",
    "\n",
    "Let's select 25% reasoning and 75% chat based:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "83d72f02",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_percentage = 0.75"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06cbe2e2",
   "metadata": {},
   "source": [
    "Let's sample the reasoning dataset by 25% (or whatever is 100% - chat_percentage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1365dce3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "non_reasoning_subset = pd.Series(non_reasoning_conversations)\n",
    "non_reasoning_subset = non_reasoning_subset.sample(\n",
    "    int(len(reasoning_conversations) * (1.0 - chat_percentage)),\n",
    "    random_state = 2407,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38525002",
   "metadata": {},
   "source": [
    "Finally combine both datasets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "80f3503a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.concat([\n",
    "    pd.Series(reasoning_conversations),\n",
    "    pd.Series(non_reasoning_subset)\n",
    "])\n",
    "data.name = \"text\"\n",
    "\n",
    "from datasets import Dataset\n",
    "combined_dataset = Dataset.from_pandas(pd.DataFrame(data))\n",
    "combined_dataset = combined_dataset.shuffle(seed = 3407)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f568d4d7",
   "metadata": {},
   "source": [
    "<a name=\"Train\"></a>\n",
    "### Train the model\n",
    "Now let's use Huggingface TRL's `SFTTrainer`! More docs here: [TRL SFT docs](https://huggingface.co/docs/trl/sft_trainer). We do 60 steps to speed things up, but you can set `num_train_epochs=1` for a full run, and turn off `max_steps=None`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9c3db892",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth: Tokenizing [\"text\"] (num_proc=32): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 24065/24065 [00:56<00:00, 424.11 examples/s]\n"
     ]
    }
   ],
   "source": [
    "from trl import SFTTrainer, SFTConfig\n",
    "trainer = SFTTrainer(\n",
    "    model = model,\n",
    "    tokenizer = tokenizer,\n",
    "    train_dataset = combined_dataset,\n",
    "    eval_dataset = None, # Can set up evaluation!\n",
    "    max_seq_length=2048,\n",
    "    args = SFTConfig(\n",
    "        dataset_text_field = \"text\",\n",
    "        per_device_train_batch_size = 2,\n",
    "        gradient_accumulation_steps = 4, # Use GA to mimic batch size!\n",
    "        warmup_steps = 5,\n",
    "        # num_train_epochs = 1, # Set this for 1 full training run.\n",
    "        max_steps = 30,\n",
    "        learning_rate = 2e-4, # Reduce to 2e-5 for long training runs\n",
    "        logging_steps = 1,\n",
    "        optim = \"adamw_8bit\",\n",
    "        weight_decay = 0.01,\n",
    "        lr_scheduler_type = \"linear\",\n",
    "        seed = 3407,\n",
    "        report_to = \"none\", # Use this for WandB etc\n",
    "    ),\n",
    ")\n",
    "# from transformers import TrainingArguments\n",
    "# trainer = SFTTrainer(\n",
    "#     model=model,\n",
    "#     train_dataset=combined_dataset,\n",
    "#     dataset_text_field=\"text\",\n",
    "#     max_seq_length=512,\n",
    "#     args=TrainingArguments(\n",
    "#         per_device_train_batch_size=2,\n",
    "#         gradient_accumulation_steps=4,\n",
    "#         num_train_epochs=1,\n",
    "#         output_dir=\"./output\",\n",
    "#         logging_steps=1,\n",
    "#     )\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7994a356",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU = NVIDIA GeForce RTX 2080 Ti. Max memory = 21.483 GB.\n",
      "7.879 GB of memory reserved.\n"
     ]
    }
   ],
   "source": [
    "# @title Show current memory stats\n",
    "gpu_stats = torch.cuda.get_device_properties(0)\n",
    "start_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
    "max_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)\n",
    "print(f\"GPU = {gpu_stats.name}. Max memory = {max_memory} GB.\")\n",
    "print(f\"{start_gpu_memory} GB of memory reserved.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9434b8ce",
   "metadata": {},
   "source": [
    "Let's train the model! To resume a training run, set `trainer.train(resume_from_checkpoint = True)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7a29f556",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n",
      "   \\\\   /|    Num examples = 24,065 | Num Epochs = 1 | Total steps = 30\n",
      "O^O/ \\_/ \\    Batch size per device = 2 | Gradient accumulation steps = 4\n",
      "\\        /    Data Parallel GPUs = 1 | Total batch size (2 x 4 x 1) = 8\n",
      " \"-____-\"     Trainable parameters = 7,667,712/8,000,000,000 (0.10% trained)\n",
      "Unsupported conversion from f16 to f16\n",
      "LLVM ERROR: Unsupported rounding mode for conversion.\n",
      "#blocked = #ttg.blocked<{sizePerThread = [4, 4], threadsPerWarp = [1, 32], warpsPerCTA = [8, 1], order = [1, 0]}>\n",
      "#blocked1 = #ttg.blocked<{sizePerThread = [1], threadsPerWarp = [32], warpsPerCTA = [8], order = [0]}>\n",
      "#blocked2 = #ttg.blocked<{sizePerThread = [1, 8], threadsPerWarp = [8, 4], warpsPerCTA = [8, 1], order = [1, 0]}>\n",
      "#blocked3 = #ttg.blocked<{sizePerThread = [8, 1], threadsPerWarp = [4, 8], warpsPerCTA = [1, 8], order = [0, 1]}>\n",
      "#shared = #ttg.swizzled_shared<{vec = 1, perPhase = 1, maxPhase = 1, order = [1, 0]}>\n",
      "#shared1 = #ttg.swizzled_shared<{vec = 1, perPhase = 1, maxPhase = 1, order = [0, 1]}>\n",
      "#smem = #ttg.shared_memory\n",
      "module attributes {\"ttg.num-ctas\" = 1 : i32, \"ttg.num-warps\" = 8 : i32, ttg.target = \"cuda:75\", \"ttg.threads-per-warp\" = 32 : i32} {\n",
      "  tt.func public @_cce_lse_forward_kernel(%arg0: !tt.ptr<f16> {tt.divisibility = 16 : i32}, %arg1: !tt.ptr<f16> {tt.divisibility = 16 : i32}, %arg2: !tt.ptr<f32> {tt.divisibility = 16 : i32}, %arg3: !tt.ptr<f32> {tt.divisibility = 16 : i32}, %arg4: !tt.ptr<i32> {tt.divisibility = 16 : i32}, %arg5: !tt.ptr<i64> {tt.divisibility = 16 : i32}, %arg6: i32, %arg7: i32 {tt.divisibility = 16 : i32}, %arg8: i32 {tt.divisibility = 16 : i32}, %arg9: i32 {tt.divisibility = 16 : i32}, %arg10: i32 {tt.divisibility = 16 : i32}, %arg11: i32 {tt.divisibility = 16 : i32}, %arg12: i32 {tt.divisibility = 16 : i32}) attributes {noinline = false} {\n",
      "    %cst = arith.constant dense<0xFF800000> : tensor<256x128xf32, #blocked>\n",
      "    %cst_0 = arith.constant dense<0.000000e+00> : tensor<256x128xf32, #blocked>\n",
      "    %cst_1 = arith.constant dense<0.000000e+00> : tensor<256xf32, #blocked1>\n",
      "    %true = arith.constant true\n",
      "    %c255_i32 = arith.constant 255 : i32\n",
      "    %c127_i32 = arith.constant 127 : i32\n",
      "    %c32_i32 = arith.constant 32 : i32\n",
      "    %c31_i32 = arith.constant 31 : i32\n",
      "    %c8_i32 = arith.constant 8 : i32\n",
      "    %c256_i32 = arith.constant 256 : i32\n",
      "    %c1_i32 = arith.constant 1 : i32\n",
      "    %c128_i32 = arith.constant 128 : i32\n",
      "    %c0_i32 = arith.constant 0 : i32\n",
      "    %cst_2 = arith.constant dense<32> : tensor<256x32xi32, #blocked2>\n",
      "    %cst_3 = arith.constant dense<32> : tensor<32x128xi32, #blocked3>\n",
      "    %0 = tt.get_program_id x : i32\n",
      "    %1 = arith.addi %arg6, %c255_i32 : i32\n",
      "    %2 = arith.divsi %1, %c256_i32 : i32\n",
      "    %3 = arith.addi %arg7, %c127_i32 : i32\n",
      "    %4 = arith.divsi %3, %c128_i32 : i32\n",
      "    %5 = arith.muli %4, %c8_i32 : i32\n",
      "    %6 = arith.divsi %0, %5 : i32\n",
      "    %7 = arith.muli %6, %c8_i32 : i32\n",
      "    %8 = arith.subi %2, %7 : i32\n",
      "    %9 = arith.minsi %8, %c8_i32 : i32\n",
      "    %10 = arith.remsi %0, %5 : i32\n",
      "    %11 = arith.remsi %10, %9 : i32\n",
      "    %12 = arith.addi %7, %11 : i32\n",
      "    %13 = arith.divsi %10, %9 : i32\n",
      "    %14 = arith.muli %12, %c256_i32 : i32\n",
      "    %15 = tt.make_range {end = 256 : i32, start = 0 : i32} : tensor<256xi32, #ttg.slice<{dim = 1, parent = #blocked}>>\n",
      "    %16 = tt.make_range {end = 256 : i32, start = 0 : i32} : tensor<256xi32, #blocked1>\n",
      "    %17 = tt.splat %14 : i32 -> tensor<256xi32, #ttg.slice<{dim = 1, parent = #blocked}>>\n",
      "    %18 = tt.splat %14 : i32 -> tensor<256xi32, #blocked1>\n",
      "    %19 = arith.addi %17, %15 : tensor<256xi32, #ttg.slice<{dim = 1, parent = #blocked}>>\n",
      "    %20 = arith.addi %18, %16 : tensor<256xi32, #blocked1>\n",
      "    %21 = tt.splat %arg6 : i32 -> tensor<256xi32, #ttg.slice<{dim = 1, parent = #blocked}>>\n",
      "    %22 = tt.splat %arg6 : i32 -> tensor<256xi32, #blocked1>\n",
      "    %23 = arith.remsi %20, %22 : tensor<256xi32, #blocked1>\n",
      "    %24 = tt.splat %arg5 : !tt.ptr<i64> -> tensor<256x!tt.ptr<i64>, #blocked1>\n",
      "    %25 = tt.addptr %24, %23 : tensor<256x!tt.ptr<i64>, #blocked1>, tensor<256xi32, #blocked1>\n",
      "    %26 = tt.load %25 : tensor<256x!tt.ptr<i64>, #blocked1>\n",
      "    %27 = arith.muli %13, %c128_i32 : i32\n",
      "    %28 = tt.make_range {end = 128 : i32, start = 0 : i32} : tensor<128xi32, #ttg.slice<{dim = 0, parent = #blocked3}>>\n",
      "    %29 = tt.make_range {end = 128 : i32, start = 0 : i32} : tensor<128xi32, #ttg.slice<{dim = 0, parent = #blocked}>>\n",
      "    %30 = tt.make_range {end = 128 : i32, start = 0 : i32} : tensor<128xi32, #blocked1>\n",
      "    %31 = tt.splat %27 : i32 -> tensor<128xi32, #ttg.slice<{dim = 0, parent = #blocked3}>>\n",
      "    %32 = tt.splat %27 : i32 -> tensor<128xi32, #ttg.slice<{dim = 0, parent = #blocked}>>\n",
      "    %33 = tt.splat %27 : i32 -> tensor<128xi32, #blocked1>\n",
      "    %34 = arith.addi %31, %28 : tensor<128xi32, #ttg.slice<{dim = 0, parent = #blocked3}>>\n",
      "    %35 = arith.addi %32, %29 : tensor<128xi32, #ttg.slice<{dim = 0, parent = #blocked}>>\n",
      "    %36 = arith.addi %33, %30 : tensor<128xi32, #blocked1>\n",
      "    %37 = tt.splat %arg7 : i32 -> tensor<128xi32, #ttg.slice<{dim = 0, parent = #blocked3}>>\n",
      "    %38 = tt.splat %arg7 : i32 -> tensor<128xi32, #ttg.slice<{dim = 0, parent = #blocked}>>\n",
      "    %39 = tt.splat %arg7 : i32 -> tensor<128xi32, #blocked1>\n",
      "    %40 = arith.remsi %34, %37 : tensor<128xi32, #ttg.slice<{dim = 0, parent = #blocked3}>>\n",
      "    %41 = arith.remsi %36, %39 : tensor<128xi32, #blocked1>\n",
      "    %42 = ttg.convert_layout %26 : tensor<256xi64, #blocked1> -> tensor<256xi64, #ttg.slice<{dim = 1, parent = #blocked2}>>\n",
      "    %43 = tt.expand_dims %42 {axis = 1 : i32} : tensor<256xi64, #ttg.slice<{dim = 1, parent = #blocked2}>> -> tensor<256x1xi64, #blocked2>\n",
      "    %44 = arith.extsi %arg9 : i32 to i64\n",
      "    %45 = tt.splat %44 : i64 -> tensor<256x1xi64, #blocked2>\n",
      "    %46 = arith.muli %43, %45 : tensor<256x1xi64, #blocked2>\n",
      "    %47 = tt.make_range {end = 32 : i32, start = 0 : i32} : tensor<32xi32, #ttg.slice<{dim = 0, parent = #blocked2}>>\n",
      "    %48 = tt.expand_dims %47 {axis = 0 : i32} : tensor<32xi32, #ttg.slice<{dim = 0, parent = #blocked2}>> -> tensor<1x32xi32, #blocked2>\n",
      "    %49 = arith.extsi %48 : tensor<1x32xi32, #blocked2> to tensor<1x32xi64, #blocked2>\n",
      "    %50 = tt.broadcast %46 : tensor<256x1xi64, #blocked2> -> tensor<256x32xi64, #blocked2>\n",
      "    %51 = tt.broadcast %49 : tensor<1x32xi64, #blocked2> -> tensor<256x32xi64, #blocked2>\n",
      "    %52 = arith.addi %50, %51 : tensor<256x32xi64, #blocked2>\n",
      "    %53 = tt.splat %arg0 : !tt.ptr<f16> -> tensor<256x32x!tt.ptr<f16>, #blocked2>\n",
      "    %54 = tt.addptr %53, %52 : tensor<256x32x!tt.ptr<f16>, #blocked2>, tensor<256x32xi64, #blocked2>\n",
      "    %55 = tt.expand_dims %40 {axis = 0 : i32} : tensor<128xi32, #ttg.slice<{dim = 0, parent = #blocked3}>> -> tensor<1x128xi32, #blocked3>\n",
      "    %56 = tt.splat %arg10 : i32 -> tensor<1x128xi32, #blocked3>\n",
      "    %57 = arith.muli %55, %56 : tensor<1x128xi32, #blocked3>\n",
      "    %58 = tt.make_range {end = 32 : i32, start = 0 : i32} : tensor<32xi32, #ttg.slice<{dim = 1, parent = #blocked3}>>\n",
      "    %59 = tt.expand_dims %58 {axis = 1 : i32} : tensor<32xi32, #ttg.slice<{dim = 1, parent = #blocked3}>> -> tensor<32x1xi32, #blocked3>\n",
      "    %60 = tt.broadcast %57 : tensor<1x128xi32, #blocked3> -> tensor<32x128xi32, #blocked3>\n",
      "    %61 = tt.broadcast %59 : tensor<32x1xi32, #blocked3> -> tensor<32x128xi32, #blocked3>\n",
      "    %62 = arith.addi %60, %61 : tensor<32x128xi32, #blocked3>\n",
      "    %63 = tt.splat %arg1 : !tt.ptr<f16> -> tensor<32x128x!tt.ptr<f16>, #blocked3>\n",
      "    %64 = tt.addptr %63, %62 : tensor<32x128x!tt.ptr<f16>, #blocked3>, tensor<32x128xi32, #blocked3>\n",
      "    %65 = arith.addi %arg8, %c31_i32 : i32\n",
      "    %66 = arith.divsi %65, %c32_i32 : i32\n",
      "    %67:3 = scf.for %arg13 = %c0_i32 to %66 step %c1_i32 iter_args(%arg14 = %cst_0, %arg15 = %54, %arg16 = %64) -> (tensor<256x128xf32, #blocked>, tensor<256x32x!tt.ptr<f16>, #blocked2>, tensor<32x128x!tt.ptr<f16>, #blocked3>)  : i32 {\n",
      "      %111 = tt.load %arg15 : tensor<256x32x!tt.ptr<f16>, #blocked2>\n",
      "      %112 = tt.load %arg16 : tensor<32x128x!tt.ptr<f16>, #blocked3>\n",
      "      %113 = tt.fp_to_fp %111 : tensor<256x32xf16, #blocked2> -> tensor<256x32xf32, #blocked2>\n",
      "      %114 = ttg.local_alloc %113 : (tensor<256x32xf32, #blocked2>) -> !ttg.memdesc<256x32xf32, #shared, #smem>\n",
      "      %115 = ttg.local_load %114 : !ttg.memdesc<256x32xf32, #shared, #smem> -> tensor<256x32xf32, #ttg.dot_op<{opIdx = 0, parent = #blocked}>>\n",
      "      %116 = tt.fp_to_fp %112 : tensor<32x128xf16, #blocked3> -> tensor<32x128xf32, #blocked3>\n",
      "      %117 = ttg.local_alloc %116 : (tensor<32x128xf32, #blocked3>) -> !ttg.memdesc<32x128xf32, #shared1, #smem>\n",
      "      %118 = ttg.local_load %117 : !ttg.memdesc<32x128xf32, #shared1, #smem> -> tensor<32x128xf32, #ttg.dot_op<{opIdx = 1, parent = #blocked}>>\n",
      "      %119 = tt.dot %115, %118, %arg14 : tensor<256x32xf32, #ttg.dot_op<{opIdx = 0, parent = #blocked}>> * tensor<32x128xf32, #ttg.dot_op<{opIdx = 1, parent = #blocked}>> -> tensor<256x128xf32, #blocked>\n",
      "      %120 = tt.addptr %arg15, %cst_2 : tensor<256x32x!tt.ptr<f16>, #blocked2>, tensor<256x32xi32, #blocked2>\n",
      "      %121 = tt.addptr %arg16, %cst_3 : tensor<32x128x!tt.ptr<f16>, #blocked3>, tensor<32x128xi32, #blocked3>\n",
      "      scf.yield %119, %120, %121 : tensor<256x128xf32, #blocked>, tensor<256x32x!tt.ptr<f16>, #blocked2>, tensor<32x128x!tt.ptr<f16>, #blocked3>\n",
      "    }\n",
      "    %68 = arith.cmpi slt, %35, %38 : tensor<128xi32, #ttg.slice<{dim = 0, parent = #blocked}>>\n",
      "    %69 = arith.cmpi slt, %36, %39 : tensor<128xi32, #blocked1>\n",
      "    %70 = tt.expand_dims %68 {axis = 0 : i32} : tensor<128xi1, #ttg.slice<{dim = 0, parent = #blocked}>> -> tensor<1x128xi1, #blocked>\n",
      "    %71 = tt.broadcast %70 : tensor<1x128xi1, #blocked> -> tensor<256x128xi1, #blocked>\n",
      "    %72 = arith.select %71, %67#0, %cst : tensor<256x128xi1, #blocked>, tensor<256x128xf32, #blocked>\n",
      "    %73 = arith.cmpi slt, %19, %21 : tensor<256xi32, #ttg.slice<{dim = 1, parent = #blocked}>>\n",
      "    %74 = arith.cmpi slt, %20, %22 : tensor<256xi32, #blocked1>\n",
      "    %75 = tt.expand_dims %73 {axis = 1 : i32} : tensor<256xi1, #ttg.slice<{dim = 1, parent = #blocked}>> -> tensor<256x1xi1, #blocked>\n",
      "    %76 = tt.broadcast %75 : tensor<256x1xi1, #blocked> -> tensor<256x128xi1, #blocked>\n",
      "    %77 = arith.select %76, %72, %cst_0 : tensor<256x128xi1, #blocked>, tensor<256x128xf32, #blocked>\n",
      "    %78 = \"tt.reduce\"(%77) <{axis = 0 : i32}> ({\n",
      "    ^bb0(%arg13: f32, %arg14: f32):\n",
      "      %111 = arith.addf %arg13, %arg14 : f32\n",
      "      tt.reduce.return %111 : f32\n",
      "    }) : (tensor<256x128xf32, #blocked>) -> tensor<128xf32, #ttg.slice<{dim = 0, parent = #blocked}>>\n",
      "    %79 = arith.sitofp %arg6 : i32 to f32\n",
      "    %80 = tt.splat %79 : f32 -> tensor<128xf32, #ttg.slice<{dim = 0, parent = #blocked}>>\n",
      "    %81 = arith.divf %78, %80 : tensor<128xf32, #ttg.slice<{dim = 0, parent = #blocked}>>\n",
      "    %82 = tt.splat %arg3 : !tt.ptr<f32> -> tensor<128x!tt.ptr<f32>, #blocked1>\n",
      "    %83 = tt.addptr %82, %41 : tensor<128x!tt.ptr<f32>, #blocked1>, tensor<128xi32, #blocked1>\n",
      "    %84 = ttg.convert_layout %81 : tensor<128xf32, #ttg.slice<{dim = 0, parent = #blocked}>> -> tensor<128xf32, #blocked1>\n",
      "    %85 = tt.atomic_rmw fadd, acq_rel, gpu, %83, %84, %69 : (tensor<128x!tt.ptr<f32>, #blocked1>, tensor<128xf32, #blocked1>, tensor<128xi1, #blocked1>) -> tensor<128xf32, #blocked1>\n",
      "    %86 = \"tt.reduce\"(%77) <{axis = 1 : i32}> ({\n",
      "    ^bb0(%arg13: f32, %arg14: f32):\n",
      "      %111 = arith.maxnumf %arg13, %arg14 : f32\n",
      "      tt.reduce.return %111 : f32\n",
      "    }) : (tensor<256x128xf32, #blocked>) -> tensor<256xf32, #ttg.slice<{dim = 1, parent = #blocked}>>\n",
      "    %87 = tt.expand_dims %86 {axis = 1 : i32} : tensor<256xf32, #ttg.slice<{dim = 1, parent = #blocked}>> -> tensor<256x1xf32, #blocked>\n",
      "    %88 = tt.broadcast %87 : tensor<256x1xf32, #blocked> -> tensor<256x128xf32, #blocked>\n",
      "    %89 = arith.subf %77, %88 : tensor<256x128xf32, #blocked>\n",
      "    %90 = math.exp %89 : tensor<256x128xf32, #blocked>\n",
      "    %91 = \"tt.reduce\"(%90) <{axis = 1 : i32}> ({\n",
      "    ^bb0(%arg13: f32, %arg14: f32):\n",
      "      %111 = arith.addf %arg13, %arg14 : f32\n",
      "      tt.reduce.return %111 : f32\n",
      "    }) : (tensor<256x128xf32, #blocked>) -> tensor<256xf32, #ttg.slice<{dim = 1, parent = #blocked}>>\n",
      "    %92 = math.log %91 : tensor<256xf32, #ttg.slice<{dim = 1, parent = #blocked}>>\n",
      "    %93 = arith.addf %86, %92 : tensor<256xf32, #ttg.slice<{dim = 1, parent = #blocked}>>\n",
      "    %94 = ttg.convert_layout %93 : tensor<256xf32, #ttg.slice<{dim = 1, parent = #blocked}>> -> tensor<256xf32, #blocked1>\n",
      "    %95 = tt.splat %arg2 : !tt.ptr<f32> -> tensor<256x!tt.ptr<f32>, #blocked1>\n",
      "    %96 = tt.addptr %95, %20 : tensor<256x!tt.ptr<f32>, #blocked1>, tensor<256xi32, #blocked1>\n",
      "    %97 = arith.muli %arg11, %c256_i32 : i32\n",
      "    %98 = arith.addi %arg6, %97 : i32\n",
      "    %99 = arith.subi %98, %c1_i32 : i32\n",
      "    %100 = arith.divsi %99, %97 : i32\n",
      "    %101 = arith.divsi %12, %100 : i32\n",
      "    %102 = tt.addptr %arg4, %101 : !tt.ptr<i32>, i32\n",
      "    scf.while : () -> () {\n",
      "      %111 = tt.atomic_cas acq_rel, gpu, %102, %c0_i32, %c1_i32 : (!tt.ptr<i32>, i32, i32) -> i32\n",
      "      %112 = arith.cmpi eq, %111, %c1_i32 : i32\n",
      "      scf.condition(%112)\n",
      "    } do {\n",
      "      scf.yield\n",
      "    }\n",
      "    %103 = tt.load %96, %74, %cst_1 evictionPolicy = evict_last : tensor<256x!tt.ptr<f32>, #blocked1>\n",
      "    %104 = arith.minnumf %103, %94 : tensor<256xf32, #blocked1>\n",
      "    %105 = arith.maxnumf %103, %94 : tensor<256xf32, #blocked1>\n",
      "    %106 = arith.subf %104, %105 : tensor<256xf32, #blocked1>\n",
      "    %107 = math.exp %106 : tensor<256xf32, #blocked1>\n",
      "    %108 = tt.extern_elementwise %107 {libname = \"\", libpath = \"\", pure = true, symbol = \"__nv_log1pf\"} : (tensor<256xf32, #blocked1>) -> tensor<256xf32, #blocked1>\n",
      "    %109 = arith.addf %108, %105 : tensor<256xf32, #blocked1>\n",
      "    tt.store %96, %109, %74 evictionPolicy = evict_last : tensor<256x!tt.ptr<f32>, #blocked1>\n",
      "    %110 = tt.atomic_rmw exch, acq_rel, gpu, %102, %c0_i32, %true : (!tt.ptr<i32>, i32, i1) -> i32\n",
      "    tt.return\n",
      "  }\n",
      "}\n",
      "\n",
      "{-#\n",
      "  external_resources: {\n",
      "    mlir_reproducer: {\n",
      "      pipeline: \"builtin.module(triton-nvidia-mma-lowering, tritongpu-combine-tensor-select-and-if, tritongpu-allocate-warp-groups, convert-scf-to-cf, allocate-shared-memory, triton-tensor-memory-allocation, tritongpu-global-scratch-memory-allocation, convert-triton-gpu-to-llvm{compute-capability=75 ptx-version=84}, canonicalize{  max-iterations=10 max-num-rewrites=-1 region-simplify=normal test-convergence=false top-down=true}, cse, convert-nv-gpu-to-llvm, convert-warp-specialize-to-llvm, canonicalize{  max-iterations=10 max-num-rewrites=-1 region-simplify=normal test-convergence=false top-down=true}, cse, symbol-dce, enable-line-info)\",\n",
      "      disable_threading: false,\n",
      "      verify_each: true\n",
      "    }\n",
      "  }\n",
      "#-}\n",
      "/home/seven/tool/miniconda3/envs/unsloth/lib/python3.12/site-packages/cut_cross_entropy/cce_lse_forward.py:12:0: error: Failures have been detected while processing an MLIR pass pipeline\n",
      "/home/seven/tool/miniconda3/envs/unsloth/lib/python3.12/site-packages/cut_cross_entropy/cce_lse_forward.py:12:0: note: Pipeline failed while executing [`ConvertTritonGPUToLLVM` on 'builtin.module' operation]: reproducer generated at `std::errs, please share the reproducer above with Triton project.`\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "PassManager::run failed",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[26]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m trainer_stats = \u001b[43mtrainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/tool/miniconda3/envs/unsloth/lib/python3.12/site-packages/transformers/trainer.py:2245\u001b[39m, in \u001b[36mTrainer.train\u001b[39m\u001b[34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[39m\n\u001b[32m   2243\u001b[39m         hf_hub_utils.enable_progress_bars()\n\u001b[32m   2244\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2245\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2246\u001b[39m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[43m=\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2247\u001b[39m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2248\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2249\u001b[39m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m=\u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2250\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<string>:314\u001b[39m, in \u001b[36m_fast_inner_training_loop\u001b[39m\u001b[34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<string>:31\u001b[39m, in \u001b[36m_unsloth_training_step\u001b[39m\u001b[34m(self, model, inputs, num_items_in_batch)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/tool/git/35_repository_for_seven/unsloth_ft_demo_20250506/unsloth_compiled_cache/UnslothSFTTrainer.py:748\u001b[39m, in \u001b[36m_UnslothSFTTrainer.compute_loss\u001b[39m\u001b[34m(self, model, inputs, return_outputs, num_items_in_batch)\u001b[39m\n\u001b[32m    747\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcompute_loss\u001b[39m(\u001b[38;5;28mself\u001b[39m, model, inputs, return_outputs = \u001b[38;5;28;01mFalse\u001b[39;00m, num_items_in_batch = \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[32m--> \u001b[39m\u001b[32m748\u001b[39m     outputs = \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcompute_loss\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    749\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    750\u001b[39m \u001b[43m        \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    751\u001b[39m \u001b[43m        \u001b[49m\u001b[43mreturn_outputs\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    752\u001b[39m \u001b[43m        \u001b[49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    753\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    754\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m outputs\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/tool/miniconda3/envs/unsloth/lib/python3.12/site-packages/unsloth/models/_utils.py:1036\u001b[39m, in \u001b[36m_unsloth_pre_compute_loss\u001b[39m\u001b[34m(self, model, inputs, *args, **kwargs)\u001b[39m\n\u001b[32m   1030\u001b[39m     logger.warning_once(\n\u001b[32m   1031\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mUnsloth: Not an error, but \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m does not accept `num_items_in_batch`.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\\\n\u001b[32m   1032\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mUsing gradient accumulation will be very slightly less accurate.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\\\n\u001b[32m   1033\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mRead more on gradient accumulation issues here: https://unsloth.ai/blog/gradient\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1034\u001b[39m     )\n\u001b[32m   1035\u001b[39m \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1036\u001b[39m outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_old_compute_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1037\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m outputs\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/tool/miniconda3/envs/unsloth/lib/python3.12/site-packages/transformers/trainer.py:3801\u001b[39m, in \u001b[36mTrainer.compute_loss\u001b[39m\u001b[34m(self, model, inputs, return_outputs, num_items_in_batch)\u001b[39m\n\u001b[32m   3799\u001b[39m         loss_kwargs[\u001b[33m\"\u001b[39m\u001b[33mnum_items_in_batch\u001b[39m\u001b[33m\"\u001b[39m] = num_items_in_batch\n\u001b[32m   3800\u001b[39m     inputs = {**inputs, **loss_kwargs}\n\u001b[32m-> \u001b[39m\u001b[32m3801\u001b[39m outputs = \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3802\u001b[39m \u001b[38;5;66;03m# Save past state if it exists\u001b[39;00m\n\u001b[32m   3803\u001b[39m \u001b[38;5;66;03m# TODO: this needs to be fixed and made cleaner later.\u001b[39;00m\n\u001b[32m   3804\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.args.past_index >= \u001b[32m0\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/tool/miniconda3/envs/unsloth/lib/python3.12/site-packages/torch/nn/modules/module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/tool/miniconda3/envs/unsloth/lib/python3.12/site-packages/torch/nn/modules/module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/tool/miniconda3/envs/unsloth/lib/python3.12/site-packages/accelerate/utils/operations.py:814\u001b[39m, in \u001b[36mconvert_outputs_to_fp32.<locals>.forward\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    813\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(*args, **kwargs):\n\u001b[32m--> \u001b[39m\u001b[32m814\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodel_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/tool/miniconda3/envs/unsloth/lib/python3.12/site-packages/accelerate/utils/operations.py:802\u001b[39m, in \u001b[36mConvertOutputsToFp32.__call__\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    801\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, *args, **kwargs):\n\u001b[32m--> \u001b[39m\u001b[32m802\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m convert_to_fp32(\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/tool/miniconda3/envs/unsloth/lib/python3.12/site-packages/torch/amp/autocast_mode.py:44\u001b[39m, in \u001b[36mautocast_decorator.<locals>.decorate_autocast\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m     41\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(func)\n\u001b[32m     42\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecorate_autocast\u001b[39m(*args, **kwargs):\n\u001b[32m     43\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m autocast_instance:\n\u001b[32m---> \u001b[39m\u001b[32m44\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/tool/miniconda3/envs/unsloth/lib/python3.12/site-packages/accelerate/utils/operations.py:814\u001b[39m, in \u001b[36mconvert_outputs_to_fp32.<locals>.forward\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    813\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(*args, **kwargs):\n\u001b[32m--> \u001b[39m\u001b[32m814\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodel_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/tool/miniconda3/envs/unsloth/lib/python3.12/site-packages/accelerate/utils/operations.py:802\u001b[39m, in \u001b[36mConvertOutputsToFp32.__call__\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    801\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, *args, **kwargs):\n\u001b[32m--> \u001b[39m\u001b[32m802\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m convert_to_fp32(\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/tool/miniconda3/envs/unsloth/lib/python3.12/site-packages/torch/amp/autocast_mode.py:44\u001b[39m, in \u001b[36mautocast_decorator.<locals>.decorate_autocast\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m     41\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(func)\n\u001b[32m     42\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecorate_autocast\u001b[39m(*args, **kwargs):\n\u001b[32m     43\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m autocast_instance:\n\u001b[32m---> \u001b[39m\u001b[32m44\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "    \u001b[31m[... skipping similar frames: ConvertOutputsToFp32.__call__ at line 802 (1 times), autocast_decorator.<locals>.decorate_autocast at line 44 (1 times), convert_outputs_to_fp32.<locals>.forward at line 814 (1 times)]\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/tool/miniconda3/envs/unsloth/lib/python3.12/site-packages/accelerate/utils/operations.py:814\u001b[39m, in \u001b[36mconvert_outputs_to_fp32.<locals>.forward\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    813\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(*args, **kwargs):\n\u001b[32m--> \u001b[39m\u001b[32m814\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodel_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/tool/miniconda3/envs/unsloth/lib/python3.12/site-packages/accelerate/utils/operations.py:802\u001b[39m, in \u001b[36mConvertOutputsToFp32.__call__\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    801\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, *args, **kwargs):\n\u001b[32m--> \u001b[39m\u001b[32m802\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m convert_to_fp32(\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/tool/miniconda3/envs/unsloth/lib/python3.12/site-packages/torch/amp/autocast_mode.py:44\u001b[39m, in \u001b[36mautocast_decorator.<locals>.decorate_autocast\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m     41\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(func)\n\u001b[32m     42\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecorate_autocast\u001b[39m(*args, **kwargs):\n\u001b[32m     43\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m autocast_instance:\n\u001b[32m---> \u001b[39m\u001b[32m44\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/tool/miniconda3/envs/unsloth/lib/python3.12/site-packages/torch/_compile.py:51\u001b[39m, in \u001b[36m_disable_dynamo.<locals>.inner\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m     48\u001b[39m     disable_fn = torch._dynamo.disable(fn, recursive)\n\u001b[32m     49\u001b[39m     fn.__dynamo_disable = disable_fn  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m51\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdisable_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/tool/miniconda3/envs/unsloth/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py:838\u001b[39m, in \u001b[36mDisableContext.__call__.<locals>._fn\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    836\u001b[39m _maybe_set_eval_frame(_callback_from_stance(\u001b[38;5;28mself\u001b[39m.callback))\n\u001b[32m    837\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m838\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    839\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    840\u001b[39m     set_eval_frame(\u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/tool/miniconda3/envs/unsloth/lib/python3.12/site-packages/unsloth/models/llama.py:1207\u001b[39m, in \u001b[36mPeftModelForCausalLM_fast_forward\u001b[39m\u001b[34m(self, input_ids, causal_mask, attention_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict, task_ids, num_logits_to_keep, logits_to_keep, **kwargs)\u001b[39m\n\u001b[32m   1191\u001b[39m \u001b[38;5;129m@torch\u001b[39m._disable_dynamo\n\u001b[32m   1192\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mPeftModelForCausalLM_fast_forward\u001b[39m(\n\u001b[32m   1193\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1205\u001b[39m     **kwargs,\n\u001b[32m   1206\u001b[39m ):\n\u001b[32m-> \u001b[39m\u001b[32m1207\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbase_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1208\u001b[39m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1209\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcausal_mask\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mcausal_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1210\u001b[39m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1211\u001b[39m \u001b[43m        \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1212\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1213\u001b[39m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1214\u001b[39m \u001b[43m        \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1215\u001b[39m \u001b[43m        \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1216\u001b[39m \u001b[43m        \u001b[49m\u001b[43mnum_logits_to_keep\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_logits_to_keep\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1217\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlogits_to_keep\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogits_to_keep\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1218\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1219\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/tool/miniconda3/envs/unsloth/lib/python3.12/site-packages/torch/nn/modules/module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/tool/miniconda3/envs/unsloth/lib/python3.12/site-packages/torch/nn/modules/module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/tool/miniconda3/envs/unsloth/lib/python3.12/site-packages/peft/tuners/tuners_utils.py:193\u001b[39m, in \u001b[36mBaseTuner.forward\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    192\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, *args: Any, **kwargs: Any):\n\u001b[32m--> \u001b[39m\u001b[32m193\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/tool/miniconda3/envs/unsloth/lib/python3.12/site-packages/unsloth/models/llama.py:1104\u001b[39m, in \u001b[36mCausalLM_fast_forward.<locals>._CausalLM_fast_forward\u001b[39m\u001b[34m(self, input_ids, causal_mask, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict, num_logits_to_keep, logits_to_keep, *args, **kwargs)\u001b[39m\n\u001b[32m   1101\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m RETURN_LOGITS \u001b[38;5;129;01mand\u001b[39;00m HAS_CUT_CROSS_ENTROPY \u001b[38;5;129;01mand\u001b[39;00m labels \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   1103\u001b[39m     n_items = kwargs.get(\u001b[33m\"\u001b[39m\u001b[33mnum_items_in_batch\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;129;01mor\u001b[39;00m kwargs.get(\u001b[33m\"\u001b[39m\u001b[33mn_items\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m-> \u001b[39m\u001b[32m1104\u001b[39m     loss = \u001b[43mfused_linear_cross_entropy\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1105\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m      \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1106\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlm_weight\u001b[49m\u001b[43m          \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mlm_head\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1107\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m             \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1108\u001b[39m \u001b[43m        \u001b[49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_items\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1109\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlogit_softcapping\u001b[49m\u001b[43m  \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogit_softcapping\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1110\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1111\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m return_dict:\n\u001b[32m   1112\u001b[39m         output = (logits,) + outputs[\u001b[32m1\u001b[39m:]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/tool/miniconda3/envs/unsloth/lib/python3.12/site-packages/unsloth_zoo/loss_utils.py:170\u001b[39m, in \u001b[36mfused_linear_cross_entropy\u001b[39m\u001b[34m(hidden_states, lm_weight, labels, num_items_in_batch, ignore_index, reduction, logit_softcapping, accuracy_threshold)\u001b[39m\n\u001b[32m    168\u001b[39m reduction = \u001b[33m\"\u001b[39m\u001b[33msum\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m num_items_in_batch \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mmean\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    169\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m logit_softcapping == \u001b[32m0\u001b[39m: logit_softcapping = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m170\u001b[39m loss = \u001b[43mlinear_cross_entropy\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    171\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlm_weight\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    172\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlm_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    173\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtargets\u001b[49m\u001b[43m      \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    174\u001b[39m \u001b[43m    \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    175\u001b[39m \u001b[43m    \u001b[49m\u001b[43msoftcap\u001b[49m\u001b[43m      \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogit_softcapping\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    176\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreduction\u001b[49m\u001b[43m    \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mreduction\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    177\u001b[39m \u001b[43m    \u001b[49m\u001b[43mshift\u001b[49m\u001b[43m        \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    178\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfilter_eps\u001b[49m\u001b[43m   \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43maccuracy_threshold\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    179\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    180\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m num_items_in_batch \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m: loss = loss / num_items_in_batch\n\u001b[32m    181\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/tool/miniconda3/envs/unsloth/lib/python3.12/site-packages/cut_cross_entropy/linear_cross_entropy.py:58\u001b[39m, in \u001b[36mlinear_cross_entropy\u001b[39m\u001b[34m(e, c, targets, ignore_index, softcap, reduction, shift, filter_eps, impl)\u001b[39m\n\u001b[32m     53\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m     54\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mCCE does not support MacOS. Please use torch_compile when running on MacOS instead.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     55\u001b[39m         )\n\u001b[32m     57\u001b[39m     \u001b[38;5;28;01massert\u001b[39;00m cce_linear_cross_entropy \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m58\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcce_linear_cross_entropy\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     59\u001b[39m \u001b[43m        \u001b[49m\u001b[43me\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtargets\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msoftcap\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreduction\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshift\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilter_eps\u001b[49m\n\u001b[32m     60\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     61\u001b[39m \u001b[38;5;28;01mcase\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mtorch_compile\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m     62\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m torch_compile_linear_cross_entropy(\n\u001b[32m     63\u001b[39m         e, c, targets, ignore_index, softcap, reduction, shift\n\u001b[32m     64\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/tool/miniconda3/envs/unsloth/lib/python3.12/site-packages/cut_cross_entropy/cce.py:168\u001b[39m, in \u001b[36mcce_linear_cross_entropy\u001b[39m\u001b[34m(e, c, targets, ignore_index, softcap, reduction, shift, filter_eps)\u001b[39m\n\u001b[32m    165\u001b[39m e = e.flatten(\u001b[32m0\u001b[39m, -\u001b[32m2\u001b[39m)\n\u001b[32m    166\u001b[39m targets = targets.flatten()\n\u001b[32m--> \u001b[39m\u001b[32m168\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mlinear_cross_entropy_apply\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    169\u001b[39m \u001b[43m    \u001b[49m\u001b[43me\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    170\u001b[39m \u001b[43m    \u001b[49m\u001b[43mc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    171\u001b[39m \u001b[43m    \u001b[49m\u001b[43mCCEParams\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    172\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtargets\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    173\u001b[39m \u001b[43m        \u001b[49m\u001b[43mvalids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    174\u001b[39m \u001b[43m        \u001b[49m\u001b[43msoftcap\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    175\u001b[39m \u001b[43m        \u001b[49m\u001b[43mreduction\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    176\u001b[39m \u001b[43m        \u001b[49m\u001b[43m_handle_eps\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilter_eps\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43me\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    177\u001b[39m \u001b[43m        \u001b[49m\u001b[43mshift\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    178\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbatch_shape\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    179\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    180\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/tool/miniconda3/envs/unsloth/lib/python3.12/site-packages/cut_cross_entropy/cce.py:125\u001b[39m, in \u001b[36mlinear_cross_entropy_apply\u001b[39m\u001b[34m(e, c, params)\u001b[39m\n\u001b[32m    120\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mlinear_cross_entropy_apply\u001b[39m(\n\u001b[32m    121\u001b[39m     e: torch.Tensor,\n\u001b[32m    122\u001b[39m     c: torch.Tensor,\n\u001b[32m    123\u001b[39m     params: CCEParams,\n\u001b[32m    124\u001b[39m ) -> torch.Tensor:\n\u001b[32m--> \u001b[39m\u001b[32m125\u001b[39m     loss = \u001b[43mLinearCrossEntropyFunction\u001b[49m\u001b[43m.\u001b[49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43me\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    126\u001b[39m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(loss, torch.Tensor)\n\u001b[32m    128\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m params.shift \u001b[38;5;129;01mand\u001b[39;00m params.reduction == \u001b[33m\"\u001b[39m\u001b[33mnone\u001b[39m\u001b[33m\"\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/tool/miniconda3/envs/unsloth/lib/python3.12/site-packages/torch/autograd/function.py:575\u001b[39m, in \u001b[36mFunction.apply\u001b[39m\u001b[34m(cls, *args, **kwargs)\u001b[39m\n\u001b[32m    572\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch._C._are_functorch_transforms_active():\n\u001b[32m    573\u001b[39m     \u001b[38;5;66;03m# See NOTE: [functorch vjp and autograd interaction]\u001b[39;00m\n\u001b[32m    574\u001b[39m     args = _functorch.utils.unwrap_dead_wrappers(args)\n\u001b[32m--> \u001b[39m\u001b[32m575\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m    577\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_setup_ctx_defined:\n\u001b[32m    578\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m    579\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mIn order to use an autograd.Function with functorch transforms \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    580\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m(vmap, grad, jvp, jacrev, ...), it must override the setup_context \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    581\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mstaticmethod. For more details, please see \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    582\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mhttps://pytorch.org/docs/main/notes/extending.func.html\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    583\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/tool/miniconda3/envs/unsloth/lib/python3.12/site-packages/cut_cross_entropy/cce.py:46\u001b[39m, in \u001b[36mLinearCrossEntropyFunction.forward\u001b[39m\u001b[34m(ctx, e, c, params)\u001b[39m\n\u001b[32m     43\u001b[39m needs_grad = e.requires_grad \u001b[38;5;129;01mor\u001b[39;00m c.requires_grad\n\u001b[32m     44\u001b[39m return_logit_avg = needs_grad \u001b[38;5;129;01mand\u001b[39;00m params.filter_eps \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m46\u001b[39m ret = \u001b[43mcce_lse_forward_kernel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     47\u001b[39m \u001b[43m    \u001b[49m\u001b[43me\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     48\u001b[39m \u001b[43m    \u001b[49m\u001b[43mc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     49\u001b[39m \u001b[43m    \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m.\u001b[49m\u001b[43mvalids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     50\u001b[39m \u001b[43m    \u001b[49m\u001b[43msoftcap\u001b[49m\u001b[43m=\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m.\u001b[49m\u001b[43msoftcap\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     51\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreturn_logit_avg\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_logit_avg\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     52\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     53\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m return_logit_avg:\n\u001b[32m     54\u001b[39m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(ret, \u001b[38;5;28mtuple\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/tool/miniconda3/envs/unsloth/lib/python3.12/site-packages/cut_cross_entropy/cce_lse_forward.py:184\u001b[39m, in \u001b[36mcce_lse_forward_kernel\u001b[39m\u001b[34m(e, c, valids, softcap, return_logit_avg)\u001b[39m\n\u001b[32m    181\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mgrid\u001b[39m(META) -> \u001b[38;5;28mtuple\u001b[39m[\u001b[38;5;28mint\u001b[39m]:\n\u001b[32m    182\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m (triton.cdiv(B, META[\u001b[33m\"\u001b[39m\u001b[33mBLOCK_B\u001b[39m\u001b[33m\"\u001b[39m]) * triton.cdiv(V, META[\u001b[33m\"\u001b[39m\u001b[33mBLOCK_V\u001b[39m\u001b[33m\"\u001b[39m]),)\n\u001b[32m--> \u001b[39m\u001b[32m184\u001b[39m \u001b[43m_cce_lse_forward_kernel\u001b[49m\u001b[43m[\u001b[49m\u001b[43mgrid\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    185\u001b[39m \u001b[43m    \u001b[49m\u001b[43me\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    186\u001b[39m \u001b[43m    \u001b[49m\u001b[43mc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    187\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlse\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m#\u001b[39;49;00m\n\u001b[32m    188\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlogit_avg\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    189\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlocks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    190\u001b[39m \u001b[43m    \u001b[49m\u001b[43mvalids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    191\u001b[39m \u001b[43m    \u001b[49m\u001b[43msoftcap\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    192\u001b[39m \u001b[43m    \u001b[49m\u001b[43mB\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    193\u001b[39m \u001b[43m    \u001b[49m\u001b[43mV\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    194\u001b[39m \u001b[43m    \u001b[49m\u001b[43mD\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m#\u001b[39;49;00m\n\u001b[32m    195\u001b[39m \u001b[43m    \u001b[49m\u001b[43me\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstride\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    196\u001b[39m \u001b[43m    \u001b[49m\u001b[43me\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstride\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m#\u001b[39;49;00m\n\u001b[32m    197\u001b[39m \u001b[43m    \u001b[49m\u001b[43mc\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstride\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    198\u001b[39m \u001b[43m    \u001b[49m\u001b[43mc\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstride\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m#\u001b[39;49;00m\n\u001b[32m    199\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlse\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstride\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    200\u001b[39m \u001b[43m    \u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mvalids\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mvalids\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstride\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    201\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnum_locks\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlocks\u001b[49m\u001b[43m.\u001b[49m\u001b[43msize\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    202\u001b[39m \u001b[43m    \u001b[49m\u001b[43mB_BIN\u001b[49m\u001b[43m=\u001b[49m\u001b[43mb_bin_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mB\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    203\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    205\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m return_logit_avg:\n\u001b[32m    206\u001b[39m     \u001b[38;5;28;01massert\u001b[39;00m logit_avg \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/tool/miniconda3/envs/unsloth/lib/python3.12/site-packages/triton/runtime/jit.py:347\u001b[39m, in \u001b[36mKernelInterface.__getitem__.<locals>.<lambda>\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    341\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, grid) -> T:\n\u001b[32m    342\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    343\u001b[39m \u001b[33;03m    A JIT function is launched with: fn[grid](*args, **kwargs).\u001b[39;00m\n\u001b[32m    344\u001b[39m \u001b[33;03m    Hence JITFunction.__getitem__ returns a callable proxy that\u001b[39;00m\n\u001b[32m    345\u001b[39m \u001b[33;03m    memorizes the grid.\u001b[39;00m\n\u001b[32m    346\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m347\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mlambda\u001b[39;00m *args, **kwargs: \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgrid\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgrid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwarmup\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/tool/miniconda3/envs/unsloth/lib/python3.12/site-packages/triton/runtime/autotuner.py:395\u001b[39m, in \u001b[36mHeuristics.run\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    393\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m v, heur \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.values.items():\n\u001b[32m    394\u001b[39m     kwargs[v] = heur({**\u001b[38;5;28mdict\u001b[39m(\u001b[38;5;28mzip\u001b[39m(\u001b[38;5;28mself\u001b[39m.arg_names, args)), **kwargs})\n\u001b[32m--> \u001b[39m\u001b[32m395\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/tool/miniconda3/envs/unsloth/lib/python3.12/site-packages/triton/runtime/autotuner.py:395\u001b[39m, in \u001b[36mHeuristics.run\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    393\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m v, heur \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.values.items():\n\u001b[32m    394\u001b[39m     kwargs[v] = heur({**\u001b[38;5;28mdict\u001b[39m(\u001b[38;5;28mzip\u001b[39m(\u001b[38;5;28mself\u001b[39m.arg_names, args)), **kwargs})\n\u001b[32m--> \u001b[39m\u001b[32m395\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/tool/miniconda3/envs/unsloth/lib/python3.12/site-packages/triton/runtime/jit.py:569\u001b[39m, in \u001b[36mJITFunction.run\u001b[39m\u001b[34m(self, grid, warmup, *args, **kwargs)\u001b[39m\n\u001b[32m    567\u001b[39m \u001b[38;5;66;03m# compile the kernel\u001b[39;00m\n\u001b[32m    568\u001b[39m src = \u001b[38;5;28mself\u001b[39m.ASTSource(\u001b[38;5;28mself\u001b[39m, signature, constexprs, attrs)\n\u001b[32m--> \u001b[39m\u001b[32m569\u001b[39m kernel = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcompile\u001b[49m\u001b[43m(\u001b[49m\u001b[43msrc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__dict__\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    570\u001b[39m kernel_cache[key] = kernel\n\u001b[32m    571\u001b[39m \u001b[38;5;28mself\u001b[39m._call_hook(key, signature, device, constexprs, options, [attrs], warmup, before=\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/tool/miniconda3/envs/unsloth/lib/python3.12/site-packages/triton/compiler/compiler.py:284\u001b[39m, in \u001b[36mcompile\u001b[39m\u001b[34m(src, target, options)\u001b[39m\n\u001b[32m    282\u001b[39m use_ir_loc = os.environ.get(\u001b[33m\"\u001b[39m\u001b[33mUSE_IR_LOC\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m    283\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m ext, compile_ir \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(stages.items())[first_stage:]:\n\u001b[32m--> \u001b[39m\u001b[32m284\u001b[39m     next_module = \u001b[43mcompile_ir\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodule\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    285\u001b[39m     ir_filename = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mext\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    286\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m (fn_override_manager \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m (full_name := fn_override_manager.get_file(ir_filename)) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/tool/miniconda3/envs/unsloth/lib/python3.12/site-packages/triton/backends/nvidia/compiler.py:450\u001b[39m, in \u001b[36mCUDABackend.add_stages.<locals>.<lambda>\u001b[39m\u001b[34m(src, metadata)\u001b[39m\n\u001b[32m    448\u001b[39m stages[\u001b[33m\"\u001b[39m\u001b[33mttir\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[38;5;28;01mlambda\u001b[39;00m src, metadata: \u001b[38;5;28mself\u001b[39m.make_ttir(src, metadata, options)\n\u001b[32m    449\u001b[39m stages[\u001b[33m\"\u001b[39m\u001b[33mttgir\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[38;5;28;01mlambda\u001b[39;00m src, metadata: \u001b[38;5;28mself\u001b[39m.make_ttgir(src, metadata, options, capability)\n\u001b[32m--> \u001b[39m\u001b[32m450\u001b[39m stages[\u001b[33m\"\u001b[39m\u001b[33mllir\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[38;5;28;01mlambda\u001b[39;00m src, metadata: \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmake_llir\u001b[49m\u001b[43m(\u001b[49m\u001b[43msrc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcapability\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    451\u001b[39m stages[\u001b[33m\"\u001b[39m\u001b[33mptx\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[38;5;28;01mlambda\u001b[39;00m src, metadata: \u001b[38;5;28mself\u001b[39m.make_ptx(src, metadata, options, \u001b[38;5;28mself\u001b[39m.target.arch)\n\u001b[32m    452\u001b[39m stages[\u001b[33m\"\u001b[39m\u001b[33mcubin\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[38;5;28;01mlambda\u001b[39;00m src, metadata: \u001b[38;5;28mself\u001b[39m.make_cubin(src, metadata, options, \u001b[38;5;28mself\u001b[39m.target.arch)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/tool/miniconda3/envs/unsloth/lib/python3.12/site-packages/triton/backends/nvidia/compiler.py:341\u001b[39m, in \u001b[36mCUDABackend.make_llir\u001b[39m\u001b[34m(self, src, metadata, options, capability)\u001b[39m\n\u001b[32m    339\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m os.environ.get(\u001b[33m\"\u001b[39m\u001b[33mTRITON_DISABLE_LINE_INFO\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33m0\u001b[39m\u001b[33m\"\u001b[39m) == \u001b[33m\"\u001b[39m\u001b[33m0\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    340\u001b[39m     passes.llvmir.add_di_scope(pm)\n\u001b[32m--> \u001b[39m\u001b[32m341\u001b[39m \u001b[43mpm\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmod\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    342\u001b[39m \u001b[38;5;66;03m# LLVM-IR (MLIR) -> LLVM-IR (LLVM)\u001b[39;00m\n\u001b[32m    343\u001b[39m llvm.init_targets()\n",
      "\u001b[31mRuntimeError\u001b[39m: PassManager::run failed"
     ]
    }
   ],
   "source": [
    "trainer_stats = trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5275aa7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "261.8103 seconds used for training.\n",
      "4.36 minutes used for training.\n",
      "Peak reserved memory = 14.066 GB.\n",
      "Peak reserved memory for training = 0.0 GB.\n",
      "Peak reserved memory % of max memory = 35.559 %.\n",
      "Peak reserved memory for training % of max memory = 0.0 %.\n"
     ]
    }
   ],
   "source": [
    "# @title Show final memory and time stats\n",
    "used_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
    "used_memory_for_lora = round(used_memory - start_gpu_memory, 3)\n",
    "used_percentage = round(used_memory / max_memory * 100, 3)\n",
    "lora_percentage = round(used_memory_for_lora / max_memory * 100, 3)\n",
    "print(f\"{trainer_stats.metrics['train_runtime']} seconds used for training.\")\n",
    "print(\n",
    "    f\"{round(trainer_stats.metrics['train_runtime']/60, 2)} minutes used for training.\"\n",
    ")\n",
    "print(f\"Peak reserved memory = {used_memory} GB.\")\n",
    "print(f\"Peak reserved memory for training = {used_memory_for_lora} GB.\")\n",
    "print(f\"Peak reserved memory % of max memory = {used_percentage} %.\")\n",
    "print(f\"Peak reserved memory for training % of max memory = {lora_percentage} %.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff0f5991",
   "metadata": {},
   "source": [
    "<a name=\"Inference\"></a>\n",
    "### Inference\n",
    "Let's run the model via Unsloth native inference! According to the `Qwen-3` team, the recommended settings for reasoning inference are `temperature = 0.6, top_p = 0.95, top_k = 20`\n",
    "\n",
    "For normal chat based inference, `temperature = 0.7, top_p = 0.8, top_k = 20`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03742a5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To solve the equation (x + 2)^2 = 0, we can take the square root of both sides. This gives us:\n",
      "\n",
      "x + 2 = 0\n",
      "\n",
      "Subtracting 2 from both sides, we get:\n",
      "\n",
      "x = -2\n",
      "\n",
      "Therefore, the solution to the equation is x = -2.<|im_end|>\n"
     ]
    }
   ],
   "source": [
    "messages = [\n",
    "    {\"role\" : \"user\", \"content\" : \"Solve (x + 2)^2 = 0.\"}\n",
    "]\n",
    "text = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize = False,\n",
    "    add_generation_prompt = True, # Must add for generation\n",
    "    enable_thinking = False, # Disable thinking\n",
    ")\n",
    "\n",
    "from transformers import TextStreamer\n",
    "_ = model.generate(\n",
    "    **tokenizer(text, return_tensors = \"pt\").to(\"cuda\"),\n",
    "    max_new_tokens = 256, # Increase for longer outputs!\n",
    "    temperature = 0.7, top_p = 0.8, top_k = 20, # For non thinking\n",
    "    streamer = TextStreamer(tokenizer, skip_prompt = True),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14db1fba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<think>\n",
      "Okay, so I need to solve the equation (x + 2)^2 = 0. Hmm, let's see. I remember that when you have something squared equals zero, the only solution is when the inside part is zero. Because if you square any real number, it's either positive or zero. So if the square is zero, the original number must be zero. So maybe I can take the square root of both sides?\n",
      "\n",
      "Wait, but the equation is (x + 2)^2 = 0. If I take the square root of both sides, that would give me x + 2 = 0, right? Because the square root of 0 is 0. Then solving for x would just be subtracting 2 from both sides, so x = -2. But wait, isn't that the only solution? Because squaring a number can't be negative, so the only way (x + 2)^2 is zero is if x + 2 is zero. So x must be -2. But since it's squared, does that mean there's a multiplicity here? Like, maybe x = -2 is a repeated root?\n",
      "\n",
      "Let me think. If we expand the equation, (x + 2)^2 = x^2 + 4x + 4. So the original equation is x^2 + 4x + 4 = 0. To solve this quadratic equation, we can factor it. Let's see, x^2 + 4x + 4 factors into (x + 2)(x + 2) = 0. So the solutions are x + 2 = 0 or x + 2 = 0. So both factors give the same solution, x = -2. Therefore, the equation has a repeated root at x = -2. So even though it's a quadratic equation, which usually has two solutions, in this case, both solutions are the same. So the answer is x = -2, but it's a double root. \n",
      "\n",
      "But the question just says \"solve\" the equation. So maybe they just want the value of x that satisfies the equation, which is -2. Even though it's a repeated root, the solution is still x = -2. So I think the answer is x = -2. Let me check again. If x = -2, then (x + 2)^2 = (-2 + 2)^2 = 0^2 = 0. Yep, that works. So that's the only solution. So the answer is x = -2. But maybe they want it in set notation or something? Like { -2 }, but probably just x = -2 is sufficient. I don't think there's any other solution here. So yeah, x = -2 is the only solution.\n",
      "</think>\n",
      "\n",
      "To solve the equation \\((x + 2)^2 = 0\\), we start by recognizing that a squared term equals zero only if the term inside the square is zero. \n",
      "\n",
      "1. Start with the given equation:\n",
      "   \\[\n",
      "   (x + 2)^2 = 0\n",
      "   \\]\n",
      "\n",
      "2. Take the square root of both sides. Since the square root of 0 is 0, we get:\n",
      "   \\[\n",
      "   x + 2 = 0\n",
      "   \\]\n",
      "\n",
      "3. Solve for \\(x\\) by subtracting 2 from both sides:\n",
      "   \\[\n",
      "   x = -2\n",
      "   \\]\n",
      "\n",
      "4. Verify the solution by substituting \\(x = -2\\) back into the original equation:\n",
      "   \\[\n",
      "   (-2 + 2)^2 = 0^2 = 0\n",
      "   \\]\n",
      "\n",
      "Thus, the solution is:\n",
      "\\[\n",
      "\\boxed{-2}\n",
      "\\]<|im_end|>\n"
     ]
    }
   ],
   "source": [
    "messages = [\n",
    "    {\"role\" : \"user\", \"content\" : \"Solve (x + 2)^2 = 0.\"}\n",
    "]\n",
    "text = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize = False,\n",
    "    add_generation_prompt = True, # Must add for generation\n",
    "    enable_thinking = True, # Disable thinking\n",
    ")\n",
    "\n",
    "from transformers import TextStreamer\n",
    "_ = model.generate(\n",
    "    **tokenizer(text, return_tensors = \"pt\").to(\"cuda\"),\n",
    "    max_new_tokens = 1024, # Increase for longer outputs!\n",
    "    temperature = 0.6, top_p = 0.95, top_k = 20, # For thinking\n",
    "    streamer = TextStreamer(tokenizer, skip_prompt = True),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca3f7457",
   "metadata": {},
   "source": [
    "<a name=\"Save\"></a>\n",
    "### Saving, loading finetuned models\n",
    "To save the final model as LoRA adapters, either use Huggingface's `push_to_hub` for an online save or `save_pretrained` for a local save.\n",
    "\n",
    "**[NOTE]** This ONLY saves the LoRA adapters, and not the full model. To save to 16bit or GGUF, scroll down!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fb4f05e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('lora_model/tokenizer_config.json',\n",
       " 'lora_model/special_tokens_map.json',\n",
       " 'lora_model/vocab.json',\n",
       " 'lora_model/merges.txt',\n",
       " 'lora_model/added_tokens.json',\n",
       " 'lora_model/tokenizer.json')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model.save_pretrained(\"lora_model\")  # Local saving\n",
    "tokenizer.save_pretrained(\"lora_model\")\n",
    "# model.push_to_hub(\"your_name/lora_model\", token = \"...\") # Online saving\n",
    "# tokenizer.push_to_hub(\"your_name/lora_model\", token = \"...\") # Online saving"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "483f490c",
   "metadata": {},
   "source": [
    "Now if you want to load the LoRA adapters we just saved for inference, set `False` to `True`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "145d432c",
   "metadata": {},
   "outputs": [],
   "source": [
    "if False:\n",
    "    from unsloth import FastLanguageModel\n",
    "    model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "        model_name = \"lora_model\", # YOUR MODEL YOU USED FOR TRAINING\n",
    "        max_seq_length = 2048,\n",
    "        load_in_4bit = True,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55a7f038",
   "metadata": {},
   "source": [
    "### Saving to float16 for VLLM\n",
    "\n",
    "We also support saving to `float16` directly. Select `merged_16bit` for float16 or `merged_4bit` for int4. We also allow `lora` adapters as a fallback. Use `push_to_hub_merged` to upload to your Hugging Face account! You can go to https://huggingface.co/settings/tokens for your personal tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07056ea4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge to 16bit\n",
    "if False:\n",
    "    model.save_pretrained_merged(\"model\", tokenizer, save_method = \"merged_16bit\",)\n",
    "if False: # Pushing to HF Hub\n",
    "    model.push_to_hub_merged(\"hf/model\", tokenizer, save_method = \"merged_16bit\", token = \"\")\n",
    "\n",
    "# Merge to 4bit\n",
    "if False:\n",
    "    model.save_pretrained_merged(\"model\", tokenizer, save_method = \"merged_4bit\",)\n",
    "if False: # Pushing to HF Hub\n",
    "    model.push_to_hub_merged(\"hf/model\", tokenizer, save_method = \"merged_4bit\", token = \"\")\n",
    "\n",
    "# Just LoRA adapters\n",
    "if False:\n",
    "    model.save_pretrained_merged(\"model\", tokenizer, save_method = \"lora\",)\n",
    "if False: # Pushing to HF Hub\n",
    "    model.push_to_hub_merged(\"hf/model\", tokenizer, save_method = \"lora\", token = \"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c483ab0",
   "metadata": {},
   "source": [
    "### GGUF / llama.cpp Conversion\n",
    "To save to `GGUF` / `llama.cpp`, we support it natively now! We clone `llama.cpp` and we default save it to `q8_0`. We allow all methods like `q4_k_m`. Use `save_pretrained_gguf` for local saving and `push_to_hub_gguf` for uploading to HF.\n",
    "\n",
    "Some supported quant methods (full list on our [Wiki page](https://github.com/unslothai/unsloth/wiki#gguf-quantization-options)):\n",
    "* `q8_0` - Fast conversion. High resource use, but generally acceptable.\n",
    "* `q4_k_m` - Recommended. Uses Q6_K for half of the attention.wv and feed_forward.w2 tensors, else Q4_K.\n",
    "* `q5_k_m` - Recommended. Uses Q6_K for half of the attention.wv and feed_forward.w2 tensors, else Q5_K.\n",
    "\n",
    "[**NEW**] To finetune and auto export to Ollama, try our [Ollama notebook](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Llama3_(8B)-Ollama.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a51653c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save to 8bit Q8_0\n",
    "if False:\n",
    "    model.save_pretrained_gguf(\"model\", tokenizer,)\n",
    "# Remember to go to https://huggingface.co/settings/tokens for a token!\n",
    "# And change hf to your username!\n",
    "if False:\n",
    "    model.push_to_hub_gguf(\"hf/model\", tokenizer, token = \"\")\n",
    "\n",
    "# Save to 16bit GGUF\n",
    "if False:\n",
    "    model.save_pretrained_gguf(\"model\", tokenizer, quantization_method = \"f16\")\n",
    "if False: # Pushing to HF Hub\n",
    "    model.push_to_hub_gguf(\"hf/model\", tokenizer, quantization_method = \"f16\", token = \"\")\n",
    "\n",
    "# Save to q4_k_m GGUF\n",
    "if False:\n",
    "    model.save_pretrained_gguf(\"model\", tokenizer, quantization_method = \"q4_k_m\")\n",
    "if False: # Pushing to HF Hub\n",
    "    model.push_to_hub_gguf(\"hf/model\", tokenizer, quantization_method = \"q4_k_m\", token = \"\")\n",
    "\n",
    "# Save to multiple GGUF options - much faster if you want multiple!\n",
    "if False:\n",
    "    model.push_to_hub_gguf(\n",
    "        \"hf/model\", # Change hf to your username!\n",
    "        tokenizer,\n",
    "        quantization_method = [\"q4_k_m\", \"q8_0\", \"q5_k_m\",],\n",
    "        token = \"\", # Get a token at https://huggingface.co/settings/tokens\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d51b098",
   "metadata": {},
   "source": [
    "Now, use the `model-unsloth.gguf` file or `model-unsloth-Q4_K_M.gguf` file in llama.cpp or a UI based system like Jan or Open WebUI. You can install Jan [here](https://github.com/janhq/jan) and Open WebUI [here](https://github.com/open-webui/open-webui)\n",
    "\n",
    "And we're done! If you have any questions on Unsloth, we have a [Discord](https://discord.gg/unsloth) channel! If you find any bugs or want to keep updated with the latest LLM stuff, or need help, join projects etc, feel free to join our Discord!\n",
    "\n",
    "Some other links:\n",
    "1. Train your own reasoning model - Llama GRPO notebook [Free Colab](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Llama3.1_(8B)-GRPO.ipynb)\n",
    "2. Saving finetunes to Ollama. [Free notebook](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Llama3_(8B)-Ollama.ipynb)\n",
    "3. Llama 3.2 Vision finetuning - Radiography use case. [Free Colab](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Llama3.2_(11B)-Vision.ipynb)\n",
    "6. See notebooks for DPO, ORPO, Continued pretraining, conversational finetuning and more on our [documentation](https://docs.unsloth.ai/get-started/unsloth-notebooks)!\n",
    "\n",
    "<div class=\"align-center\">\n",
    "  <a href=\"https://unsloth.ai\"><img src=\"https://github.com/unslothai/unsloth/raw/main/images/unsloth%20new%20logo.png\" width=\"115\"></a>\n",
    "  <a href=\"https://discord.gg/unsloth\"><img src=\"https://github.com/unslothai/unsloth/raw/main/images/Discord.png\" width=\"145\"></a>\n",
    "  <a href=\"https://docs.unsloth.ai/\"><img src=\"https://github.com/unslothai/unsloth/blob/main/images/documentation%20green%20button.png?raw=true\" width=\"125\"></a>\n",
    "\n",
    "  Join Discord if you need help + ‚≠êÔ∏è <i>Star us on <a href=\"https://github.com/unslothai/unsloth\">Github</a> </i> ‚≠êÔ∏è\n",
    "</div>\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "unsloth",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
